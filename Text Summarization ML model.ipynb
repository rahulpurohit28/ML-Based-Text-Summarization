{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries - \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize \n",
    "import gensim.models.word2vec as w2v \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,roc_auc_score,confusion_matrix,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to store the names of all the variables(objects) created \n",
    "variable_names = pd.DataFrame(columns = ['Name','info'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, info]\n",
       "Index: []"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read each category inependently - \n",
    "def read_data(cat,n):\n",
    "    sent = []\n",
    "    for i in range(1,int(n)+1):\n",
    "        if(i<10):\n",
    "            ad = \"00\" + str(i)\n",
    "        elif(i<100):\n",
    "            ad = \"0\" + str(i)\n",
    "        \n",
    "        else:\n",
    "            ad = str(i)\n",
    "        f = open(\"BBC News Summary/News Articles/\"+ cat +\"/\"+ad+\".txt\")\n",
    "        n = \"\"\n",
    "        for line in f: \n",
    "            n += line\n",
    "        sent.append(n)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a seperate  corpus for each category - \n",
    "sentence_sport = read_data('sport',511)\n",
    "sentence_tech = read_data('tech',401)\n",
    "sentence_politics = read_data('politics',417)\n",
    "sentence_business = read_data('business',510)\n",
    "sentence_entertainment = read_data('entertainment',386)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names= variable_names.append(pd.DataFrame({'Name':['sentence_sport','sentence_tech','sentence_politics',\n",
    "                                            'sentence_business','sentence_entertainment'],\n",
    "                                    'info': 'Corpus file'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name         info\n",
       "0          sentence_sport  Corpus file\n",
       "1           sentence_tech  Corpus file\n",
       "2       sentence_politics  Corpus file\n",
       "3       sentence_business  Corpus file\n",
       "4  sentence_entertainment  Corpus file"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claxton hunting first major medal\\n\\nBritish hurdler Sarah Claxton is confident she can win her first major medal at next month\\'s European Indoor Championships in Madrid.\\n\\nThe 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week\\'s Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\\n\\nFor the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sport[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ink helps drive democracy in Asia\\n\\nThe Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country\\'s elections as part of a drive to prevent multiple voting.\\n\\nThis new technology is causing both worries and guarded optimism among different sectors of the population. In an effort to live up to its reputation in the 1990s as \"an island of democracy\", the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections. The US government agreed to fund all expenses associated with this decision.\\n\\nThe Kyrgyz Republic is seen by many experts as backsliding from the high point it reached in the mid-1990s with a hastily pushed through referendum in 2003, reducing the legislative branch to one chamber with 75 deputies. The use of ink is only one part of a general effort to show commitment towards more open elections - the German Embassy, the Soros Foundation and the Kyrgyz government have all contributed to purchase transparent ballot boxes.\\n\\nThe actual technology behind the ink is not that complicated. The ink is sprayed on a person\\'s left thumb. It dries and is not visible under normal light.\\n\\nHowever, the presence of ultraviolet light (of the kind used to verify money) causes the ink to glow with a neon yellow light. At the entrance to each polling station, one election official will scan voter\\'s fingers with UV lamp before allowing them to enter, and every voter will have his/her left thumb sprayed with ink before receiving the ballot. If the ink shows under the UV light the voter will not be allowed to enter the polling station. Likewise, any voter who refuses to be inked will not receive the ballot. These elections are assuming even greater significance because of two large factors - the upcoming parliamentary elections are a prelude to a potentially regime changing presidential election in the Autumn as well as the echo of recent elections in other former Soviet Republics, notably Ukraine and Georgia. The use of ink has been controversial - especially among groups perceived to be pro-government.\\n\\nWidely circulated articles compared the use of ink to the rural practice of marking sheep - a still common metaphor in this primarily agricultural society.\\n\\nThe author of one such article began a petition drive against the use of the ink. The greatest part of the opposition to ink has often been sheer ignorance. Local newspapers have carried stories that the ink is harmful, radioactive or even that the ultraviolet readers may cause health problems. Others, such as the aggressively middle of the road, Coalition of Non-governmental Organizations, have lauded the move as an important step forward. This type of ink has been used in many elections in the world, in countries as varied as Serbia, South Africa, Indonesia and Turkey. The other common type of ink in elections is indelible visible ink - but as the elections in Afghanistan showed, improper use of this type of ink can cause additional problems. The use of \"invisible\" ink is not without its own problems. In most elections, numerous rumors have spread about it.\\n\\nIn Serbia, for example, both Christian and Islamic leaders assured their populations that its use was not contrary to religion. Other rumours are associated with how to remove the ink - various soft drinks, solvents and cleaning products are put forward. However, in reality, the ink is very effective at getting under the cuticle of the thumb and difficult to wash off. The ink stays on the finger for at least 72 hours and for up to a week. The use of ink and readers by itself is not a panacea for election ills. The passage of the inking law is, nevertheless, a clear step forward towards free and fair elections.\" The country\\'s widely watched parliamentary elections are scheduled for 27 February.\\n\\nDavid Mikosz works for the IFES, an international, non-profit organisation that supports the building of democratic societies.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tech[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store summary sentences for each title - (Input similar to read_data func)\n",
    "# Return a dictionary \n",
    "def read_data_summary(cat,n):\n",
    "  \n",
    "    summary_dict = {}\n",
    "    for i in range(1,int(n)+1):\n",
    "        sent = ''\n",
    "        if(i<10):\n",
    "            ad = \"00\" + str(i)\n",
    "        elif(i<100):\n",
    "            ad = \"0\" + str(i)\n",
    "        \n",
    "        else:\n",
    "            ad = str(i)\n",
    "        f = open(\"BBC News Summary/Summaries/\"+ cat +\"/\"+ad+\".txt\")\n",
    "        #n = []\n",
    "        for line in f: \n",
    "            sent = sent+line\n",
    "            #sent.append(line)\n",
    "        \n",
    "        summary_dict[int(ad)] = [re.sub('\"','',i) for i in sent.split(\".\") if i !='']\n",
    "    \n",
    "    return (summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating summary sentences dictionary for each each title , category wise- \n",
    "\n",
    "summary_dict_sport = read_data_summary('sport',511)\n",
    "summary_dict_tech = read_data_summary('tech',401)\n",
    "summary_dict_politics = read_data_summary('politics',417)\n",
    "summary_dict_business = read_data_summary('business',510)\n",
    "summary_dict_entertainment = read_data_summary('entertainment',386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names= variable_names.append(pd.DataFrame({'Name':['summary_dict_sport','summary_dict_tech','summary_dict_politics',\n",
    "                                            'summary_dict_business','summary_dict_entertainment'],\n",
    "                                    'info': 'Dictionary file- Key = file_no,values=Summary sentences'}),ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0              sentence_sport   \n",
       "1               sentence_tech   \n",
       "2           sentence_politics   \n",
       "3           sentence_business   \n",
       "4      sentence_entertainment   \n",
       "5          summary_dict_sport   \n",
       "6           summary_dict_tech   \n",
       "7       summary_dict_politics   \n",
       "8       summary_dict_business   \n",
       "9  summary_dict_entertainment   \n",
       "\n",
       "                                                      info  \n",
       "0                                              Corpus file  \n",
       "1                                              Corpus file  \n",
       "2                                              Corpus file  \n",
       "3                                              Corpus file  \n",
       "4                                              Corpus file  \n",
       "5  Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6  Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7  Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8  Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9  Dictionary file- Key = file_no,values=Summary sentences  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of Summary dictionaries - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form',\n",
       " 'Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage',\n",
       " \"British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid\",\n",
       " 'Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March',\n",
       " 'I am quite confident, said Claxton']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dict_sport[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The other common type of ink in elections is indelible visible ink - but as the elections in Afghanistan showed, improper use of this type of ink can cause additional problems',\n",
       " 'The use of ink and readers by itself is not a panacea for election ills',\n",
       " 'The use of invisible ink is not without its own problems',\n",
       " 'The use of ink is only one part of a general effort to show commitment towards more open elections - the German Embassy, the Soros Foundation and the Kyrgyz government have all contributed to purchase transparent ballot boxes',\n",
       " 'The author of one such article began a petition drive against the use of the ink',\n",
       " 'The use of ink has been controversial - especially among groups perceived to be pro-government',\n",
       " 'In an effort to live up to its reputation in the 1990s as an island of democracy, the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections',\n",
       " \"At the entrance to each polling station, one election official will scan voter's fingers with UV lamp before allowing them to enter, and every voter will have his/her left thumb sprayed with ink before receiving the ballot\",\n",
       " \"The ink is sprayed on a person's left thumb\",\n",
       " 'If the ink shows under the UV light the voter will not be allowed to enter the polling station',\n",
       " 'The actual technology behind the ink is not that complicated',\n",
       " \"The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting\",\n",
       " 'This type of ink has been used in many elections in the world, in countries as varied as Serbia, South Africa, Indonesia and Turkey']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dict_tech[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint 1! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make thematic words dictionary,tokens dictionary,word count dictionary for all the categories combined \n",
    "# Takes the corpus as argument, and then adds the info to a common dictionary \n",
    "them_dict={}\n",
    "token_dict={}\n",
    "word_count_dict={}\n",
    "\n",
    "def token_func(corpus):\n",
    "\n",
    "    #them_dict={}\n",
    "    #token_dict={}\n",
    "    #word_count_dict={}\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    for i in corpus:\n",
    "        title= i.split('\\n')[0]\n",
    "        \n",
    "        file_no = 1\n",
    "        body = i.split(\"\\n\")[1:]   # body is a list of sentences \n",
    "        \n",
    "        clean = re.sub('[^a-zA-Z]',' ', \" \".join(body))\n",
    "        \n",
    "        \n",
    "        tokens = word_tokenize(clean)\n",
    "        clean_list = list(lemmatizer.lemmatize(i).lower() for i in tokens if i.lower() not in stopwords.words('english') )\n",
    "        thematic_words = list(pd.Series(clean_list).value_counts().keys()[:10])\n",
    "        them_dict[title] = thematic_words\n",
    "        total_words = len(clean_list)\n",
    "        \n",
    "        token_dict[title] = clean_list\n",
    "        \n",
    "        word_count_dict[title] = total_words\n",
    "            \n",
    "        file_no += 1\n",
    "    return(them_dict,token_dict,word_count_dict)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sports corpus - \n",
    "them_dict,token_dict,word_count_dict = token_func(sentence_sport)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names= variable_names.append(pd.DataFrame({'Name':['them_dict','token_dict','word_count_dict'],\n",
    "                                                    'info':['Dict file: key = title ,values = Thematic words for each title(includes all categories)',\n",
    "                                                           'Dict file: key = title ,values = Tokens for each title(includes all categories)',\n",
    "                                                           'Dict file: key = title ,values = Total no of tokens for each title(includes all categories)']},\n",
    "                                                  ),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them_dict</td>\n",
       "      <td>Dict file: key = title ,values = Thematic words for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token_dict</td>\n",
       "      <td>Dict file: key = title ,values = Tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word_count_dict</td>\n",
       "      <td>Dict file: key = title ,values = Total no of tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name  \\\n",
       "0               sentence_sport   \n",
       "1                sentence_tech   \n",
       "2            sentence_politics   \n",
       "3            sentence_business   \n",
       "4       sentence_entertainment   \n",
       "5           summary_dict_sport   \n",
       "6            summary_dict_tech   \n",
       "7        summary_dict_politics   \n",
       "8        summary_dict_business   \n",
       "9   summary_dict_entertainment   \n",
       "10                   them_dict   \n",
       "11                  token_dict   \n",
       "12             word_count_dict   \n",
       "\n",
       "                                                                                           info  \n",
       "0                                                                                   Corpus file  \n",
       "1                                                                                   Corpus file  \n",
       "2                                                                                   Corpus file  \n",
       "3                                                                                   Corpus file  \n",
       "4                                                                                   Corpus file  \n",
       "5                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "10      Dict file: key = title ,values = Thematic words for each title(includes all categories)  \n",
       "11              Dict file: key = title ,values = Tokens for each title(includes all categories)  \n",
       "12  Dict file: key = title ,values = Total no of tokens for each title(includes all categories)  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(them_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "them_dict,token_dict,word_count_dict = token_func(sentence_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(them_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "them_dict,token_dict,word_count_dict = token_func(sentence_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "them_dict,token_dict,word_count_dict = token_func(sentence_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "them_dict,token_dict,word_count_dict = token_func(sentence_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the number of thematic words in a sentence\n",
    "def thematic_count(x,title):\n",
    "    count=0\n",
    "    for i in x:\n",
    "        if i in them_dict[title]:\n",
    "            count+=1\n",
    "    \n",
    "    return(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint 2!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate the target label - (Binary)\n",
    "def target_var(x,file_no,summary_dict):\n",
    "    if x in summary_dict[file_no]:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find length\n",
    "def sent_length(x):\n",
    "    return (len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create tokens from each sentence\n",
    "def sent_wise_token(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean = re.sub('[^a-zA-Z]',' ', x)\n",
    "    tokens = word_tokenize(clean)\n",
    "    return(list(lemmatizer.lemmatize(i).lower() for i in tokens if i.lower() not in stopwords.words('english') ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create data frame from a corpus - \n",
    "\n",
    "def create_dataframe(corpus,summary_dict):    # Output from the read_data func\n",
    "    main_data=pd.DataFrame(columns= ['Sentence','Title','Summary','Sentence_len','Sent_no'])   \n",
    "    \n",
    "    file_no= 1 \n",
    "    sent_count_dict = {}\n",
    "\n",
    "    for i in corpus:\n",
    "        title= i.split('\\n')[0]\n",
    "       \n",
    "        para= [k for k in i.split('\\n')[1:] if k!=\" \"]\n",
    "        \n",
    "        #para = [k for k in (\"\".join(para)).split('.') if k!='']\n",
    "        para = [ re.sub('\"','',i) for i in \" \".join(para).split('. ') if i !='']\n",
    "       \n",
    "                      \n",
    "        \n",
    "        count= 0          # counter for no of sentences\n",
    "        \n",
    "                \n",
    "        for j in para:\n",
    "        \n",
    "            add_sent = [ re.sub('\\.$',' ',i).strip() for i in j.split('. ') ]\n",
    "            if len(add_sent)==1:\n",
    "                \n",
    "                new= pd.DataFrame({'Sentence':[ re.sub('\\.$',' ',i).strip() for i in j.split('. ') if i!=\"\"],'Title':title,\n",
    "                                  'Sent_no' : count + 1})\n",
    "            else:\n",
    "      \n",
    "                new=pd.DataFrame({'Sentence':[ re.sub('\\.$',' ',i).strip() for i in j.split('. ')if i!=\"\"],'Title':title,\n",
    "                                  'Sent_no' : [ count+1 for i in j.split('. ') if i!=\"\" ]})\n",
    "                count=count+1\n",
    "                \n",
    "            new['Sentence_len'] = new.Sentence.apply(sent_length)\n",
    "            \n",
    "            new['Summary'] = new.Sentence.apply(target_var, args=(file_no,summary_dict,))\n",
    "            \n",
    "            new['Sent_tokens'] = new.Sentence.apply(sent_wise_token)\n",
    "            \n",
    "            new['Thematic_count'] = new.Sent_tokens.apply(thematic_count,args=(title,))\n",
    "            \n",
    "                \n",
    "                       \n",
    "            main_data = main_data.append(new,ignore_index= True, sort=False)\n",
    "            count = count + 1\n",
    "        \n",
    "                  \n",
    "        sent_count_dict[title] = count\n",
    "        file_no += 1 \n",
    "        \n",
    "    main_data['Thematic_count_norm'] = main_data.Thematic_count/main_data.Sentence_len  \n",
    "        \n",
    "         \n",
    "            \n",
    "    return (main_data,sent_count_dict) # return dataframe , and sentence count dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_sport,sent_count_dict_sport= create_dataframe(sentence_sport,summary_dict_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>[british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>[year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              Sentence  \\\n",
       "0                 British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid   \n",
       "1  The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Claxton hunting first major medal       1           21       1   \n",
       "1  Claxton hunting first major medal       0           26       2   \n",
       "\n",
       "                                                                                                                     Sent_tokens  \\\n",
       "0  [british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]   \n",
       "1            [year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             5.0            0.238095  \n",
       "1             4.0            0.153846  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td></td>\n",
       "      <td>Radcliffe enjoys winning comeback</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td></td>\n",
       "      <td>Gallas sees two-horse race</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td></td>\n",
       "      <td>Ireland 21-19 Argentina</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6501</th>\n",
       "      <td></td>\n",
       "      <td>England claim Dubai Sevens glory</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence                              Title Summary Sentence_len Sent_no  \\\n",
       "585            Radcliffe enjoys winning comeback       0            1      12   \n",
       "3511                  Gallas sees two-horse race       1            1      16   \n",
       "6490                     Ireland 21-19 Argentina       0            1      20   \n",
       "6501            England claim Dubai Sevens glory       0            1      11   \n",
       "\n",
       "     Sent_tokens  Thematic_count Thematic_count_norm  \n",
       "585           []             0.0                   0  \n",
       "3511          []             0.0                   0  \n",
       "6490          []             0.0                   0  \n",
       "6501          []             0.0                   0  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport[main_data_sport.Sentence==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some null sentences, finding the respective titles , and then reducing the number of sentence by one \n",
    "\n",
    "title_null_sent = main_data_sport.loc[main_data_sport[main_data_sport.Sentence==\"\"].index,'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([585, 3511, 6490, 6501], dtype='int64')"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_null_sent.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td></td>\n",
       "      <td>Radcliffe enjoys winning comeback</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td></td>\n",
       "      <td>Gallas sees two-horse race</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td></td>\n",
       "      <td>Ireland 21-19 Argentina</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6501</th>\n",
       "      <td></td>\n",
       "      <td>England claim Dubai Sevens glory</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence                              Title Summary Sentence_len Sent_no  \\\n",
       "585            Radcliffe enjoys winning comeback       0            1      12   \n",
       "3511                  Gallas sees two-horse race       1            1      16   \n",
       "6490                     Ireland 21-19 Argentina       0            1      20   \n",
       "6501            England claim Dubai Sevens glory       0            1      11   \n",
       "\n",
       "     Sent_tokens  Thematic_count Thematic_count_norm  \n",
       "585           []             0.0                   0  \n",
       "3511          []             0.0                   0  \n",
       "6490          []             0.0                   0  \n",
       "6501          []             0.0                   0  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.loc[title_null_sent.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in title_null_sent.value_counts().keys():\n",
    "    sent_count_dict_sport[i] = sent_count_dict_sport[i] - title_null_sent.value_counts()[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_count_dict_sport['Radcliffe enjoys winning comeback']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td></td>\n",
       "      <td>Radcliffe enjoys winning comeback</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td></td>\n",
       "      <td>Gallas sees two-horse race</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td></td>\n",
       "      <td>Ireland 21-19 Argentina</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6501</th>\n",
       "      <td></td>\n",
       "      <td>England claim Dubai Sevens glory</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence                              Title Summary Sentence_len Sent_no  \\\n",
       "585            Radcliffe enjoys winning comeback       0            1      12   \n",
       "3511                  Gallas sees two-horse race       1            1      16   \n",
       "6490                     Ireland 21-19 Argentina       0            1      20   \n",
       "6501            England claim Dubai Sevens glory       0            1      11   \n",
       "\n",
       "     Sent_tokens  Thematic_count Thematic_count_norm  \n",
       "585           []             0.0                   0  \n",
       "3511          []             0.0                   0  \n",
       "6490          []             0.0                   0  \n",
       "6501          []             0.0                   0  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport[main_data_sport.Sentence==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_sport = main_data_sport.drop(index=title_null_sent.index,axis=0)  # Removing blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_sport.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence, Title, Summary, Sentence_len, Sent_no, Sent_tokens, Thematic_count, Thematic_count_norm]\n",
       "Index: []"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport[main_data_sport.Sentence==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_tech,sent_count_dict_tech = create_dataframe(sentence_tech,summary_dict_tech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove NULL sentences and update the Sent count dictionary \n",
    "def remove_null(dataframe,sent_count_dict):\n",
    "    title_null_sent = dataframe.loc[dataframe[dataframe.Sentence==\"\"].index,'Title']\n",
    "    \n",
    "    for i in title_null_sent.value_counts().keys():\n",
    "        sent_count_dict[i] = sent_count_dict[i] - title_null_sent.value_counts()[i] \n",
    "    \n",
    "    dataframe = dataframe.drop(index=title_null_sent.index,axis=0)  # Removing blank sentences\n",
    "    dataframe.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_null(main_data_tech,sent_count_dict_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9524</th>\n",
       "      <td>Most MMORPG games you need a credit card to play but I dont think parents know just what they are letting there children into</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>138</td>\n",
       "      <td>[mmorpg, game, need, credit, card, play, dont, think, parent, know, letting, child]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9525</th>\n",
       "      <td>Unless there is undeniable medical proof that staring at a computer screens for hours at a time can damage a person&amp;#191;s health, you can expect this not to decline but to get worse</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>139</td>\n",
       "      <td>[unless, undeniable, medical, proof, staring, computer, screen, hour, time, damage, person, health, expect, decline, get, worse]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9526</th>\n",
       "      <td>These people are pathetic</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>[people, pathetic]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9527</th>\n",
       "      <td>They need to get off their machines and notice that our world is being swiftly overcome by issues and troubles that make the trifling worries of and online universe absolutely meaningless</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>141</td>\n",
       "      <td>[need, get, machine, notice, world, swiftly, overcome, issue, trouble, make, trifling, worry, online, universe, absolutely, meaningless]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0645161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9528</th>\n",
       "      <td>24hours, when i was a kid at school and i was on half term, Ultima Online was the game, ahhhh them was the days ! LOL</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>142</td>\n",
       "      <td>[hour, kid, school, half, term, ultima, online, game, ahhhh, day, lol]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                         Sentence  \\\n",
       "9524                                                                Most MMORPG games you need a credit card to play but I dont think parents know just what they are letting there children into   \n",
       "9525       Unless there is undeniable medical proof that staring at a computer screens for hours at a time can damage a person&#191;s health, you can expect this not to decline but to get worse   \n",
       "9526                                                                                                                                                                    These people are pathetic   \n",
       "9527  They need to get off their machines and notice that our world is being swiftly overcome by issues and troubles that make the trifling worries of and online universe absolutely meaningless   \n",
       "9528                                                                        24hours, when i was a kid at school and i was on half term, Ultima Online was the game, ahhhh them was the days ! LOL   \n",
       "\n",
       "                                 Title Summary Sentence_len Sent_no  \\\n",
       "9524  Losing yourself in online gaming       1           24     138   \n",
       "9525  Losing yourself in online gaming       0           33     139   \n",
       "9526  Losing yourself in online gaming       0            4     140   \n",
       "9527  Losing yourself in online gaming       0           31     141   \n",
       "9528  Losing yourself in online gaming       0           26     142   \n",
       "\n",
       "                                                                                                                                   Sent_tokens  \\\n",
       "9524                                                       [mmorpg, game, need, credit, card, play, dont, think, parent, know, letting, child]   \n",
       "9525          [unless, undeniable, medical, proof, staring, computer, screen, hour, time, damage, person, health, expect, decline, get, worse]   \n",
       "9526                                                                                                                        [people, pathetic]   \n",
       "9527  [need, get, machine, notice, world, swiftly, overcome, issue, trouble, make, trifling, worry, online, universe, absolutely, meaningless]   \n",
       "9528                                                                    [hour, kid, school, half, term, ultima, online, game, ahhhh, day, lol]   \n",
       "\n",
       "      Thematic_count Thematic_count_norm  \n",
       "9524             2.0           0.0833333  \n",
       "9525             2.0           0.0606061  \n",
       "9526             1.0                0.25  \n",
       "9527             2.0           0.0645161  \n",
       "9528             3.0            0.115385  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_tech.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9529, 8)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_tech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_politics,sent_count_dict_politics = create_dataframe(sentence_politics,summary_dict_politics)\n",
    "remove_null(main_data_politics,sent_count_dict_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_business,sent_count_dict_business = create_dataframe(sentence_business,summary_dict_business)\n",
    "remove_null(main_data_business,sent_count_dict_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_entertainment,sent_count_dict_entertainment = create_dataframe(sentence_entertainment,summary_dict_entertainment)\n",
    "remove_null(main_data_entertainment,sent_count_dict_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names= variable_names.append(pd.DataFrame({'Name':['main_data_sport','main_data_tech','main_data_politics',\n",
    "                                                            'main_data_business','main_data_entertainment'],\n",
    "                                                    'info':'Data Frame for each category'},\n",
    "                                                  ),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names= variable_names.append(pd.DataFrame({'Name':['sent_count_dict_sport','sent_count_dict_tech','sent_count_dict_politics',\n",
    "                                                            'sent_count_dict_business','sent_count_dict_entertainment'],\n",
    "                                                    'info':'Dict for each category- key= Title , values =No of sentences in each title'},\n",
    "                                                  ),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them_dict</td>\n",
       "      <td>Dict file: key = title ,values = Thematic words for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token_dict</td>\n",
       "      <td>Dict file: key = title ,values = Tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word_count_dict</td>\n",
       "      <td>Dict file: key = title ,values = Total no of tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>main_data_sport</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>main_data_tech</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>main_data_politics</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>main_data_business</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>main_data_entertainment</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sent_count_dict_sport</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sent_count_dict_tech</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sent_count_dict_politics</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sent_count_dict_business</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sent_count_dict_entertainment</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                  sentence_sport   \n",
       "1                   sentence_tech   \n",
       "2               sentence_politics   \n",
       "3               sentence_business   \n",
       "4          sentence_entertainment   \n",
       "5              summary_dict_sport   \n",
       "6               summary_dict_tech   \n",
       "7           summary_dict_politics   \n",
       "8           summary_dict_business   \n",
       "9      summary_dict_entertainment   \n",
       "10                      them_dict   \n",
       "11                     token_dict   \n",
       "12                word_count_dict   \n",
       "13                main_data_sport   \n",
       "14                 main_data_tech   \n",
       "15             main_data_politics   \n",
       "16             main_data_business   \n",
       "17        main_data_entertainment   \n",
       "18          sent_count_dict_sport   \n",
       "19           sent_count_dict_tech   \n",
       "20       sent_count_dict_politics   \n",
       "21       sent_count_dict_business   \n",
       "22  sent_count_dict_entertainment   \n",
       "\n",
       "                                                                                           info  \n",
       "0                                                                                   Corpus file  \n",
       "1                                                                                   Corpus file  \n",
       "2                                                                                   Corpus file  \n",
       "3                                                                                   Corpus file  \n",
       "4                                                                                   Corpus file  \n",
       "5                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "10      Dict file: key = title ,values = Thematic words for each title(includes all categories)  \n",
       "11              Dict file: key = title ,values = Tokens for each title(includes all categories)  \n",
       "12  Dict file: key = title ,values = Total no of tokens for each title(includes all categories)  \n",
       "13                                                                 Data Frame for each category  \n",
       "14                                                                 Data Frame for each category  \n",
       "15                                                                 Data Frame for each category  \n",
       "16                                                                 Data Frame for each category  \n",
       "17                                                                 Data Frame for each category  \n",
       "18                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "19                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "20                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "21                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "22                   Dict for each category- key= Title , values =No of sentences in each title  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>[british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>[year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am quite confident, said Claxton</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>[quite, confident, said, claxton]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But I take each race as it comes</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>[take, race, come]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As long as I keep up my training but not do too much I think there is a chance of a medal</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>[long, keep, training, much, think, chance, medal]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              Sentence  \\\n",
       "0                 British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid   \n",
       "1  The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title   \n",
       "2                                                                                                                   I am quite confident, said Claxton   \n",
       "3                                                                                                                     But I take each race as it comes   \n",
       "4                                                            As long as I keep up my training but not do too much I think there is a chance of a medal   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Claxton hunting first major medal       1           21       1   \n",
       "1  Claxton hunting first major medal       0           26       2   \n",
       "2  Claxton hunting first major medal       1            6       3   \n",
       "3  Claxton hunting first major medal       0            8       4   \n",
       "4  Claxton hunting first major medal       0           22       5   \n",
       "\n",
       "                                                                                                                     Sent_tokens  \\\n",
       "0  [british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]   \n",
       "1            [year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]   \n",
       "2                                                                                              [quite, confident, said, claxton]   \n",
       "3                                                                                                             [take, race, come]   \n",
       "4                                                                             [long, keep, training, much, think, chance, medal]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             5.0            0.238095  \n",
       "1             4.0            0.153846  \n",
       "2             2.0            0.333333  \n",
       "3             1.0               0.125  \n",
       "4             2.0           0.0909091  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>[kyrgyz, republic, small, mountainous, state, former, soviet, republic, using, invisible, ink, ultraviolet, reader, country, election, part, drive, prevent, multiple, voting]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This new technology is causing both worries and guarded optimism among different sectors of the population</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[new, technology, causing, worry, guarded, optimism, among, different, sector, population]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In an effort to live up to its reputation in the 1990s as an island of democracy, the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>[effort, live, reputation, island, democracy, kyrgyz, president, askar, akaev, pushed, law, requiring, use, ink, upcoming, parliamentary, presidential, election]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The US government agreed to fund all expenses associated with this decision</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>[us, government, agreed, fund, expense, associated, decision]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Kyrgyz Republic is seen by many experts as backsliding from the high point it reached in the mid-1990s with a hastily pushed through referendum in 2003, reducing the legislative branch to one chamber with 75 deputies</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>[kyrgyz, republic, seen, many, expert, backsliding, high, point, reached, mid, hastily, pushed, referendum, reducing, legislative, branch, one, chamber, deputy]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0540541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                            Sentence  \\\n",
       "0                             The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting   \n",
       "1                                                                                                                         This new technology is causing both worries and guarded optimism among different sectors of the population   \n",
       "2  In an effort to live up to its reputation in the 1990s as an island of democracy, the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections   \n",
       "3                                                                                                                                                        The US government agreed to fund all expenses associated with this decision   \n",
       "4       The Kyrgyz Republic is seen by many experts as backsliding from the high point it reached in the mid-1990s with a hastily pushed through referendum in 2003, reducing the legislative branch to one chamber with 75 deputies   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Ink helps drive democracy in Asia       1           32       1   \n",
       "1  Ink helps drive democracy in Asia       0           16       2   \n",
       "2  Ink helps drive democracy in Asia       1           38       3   \n",
       "3  Ink helps drive democracy in Asia       0           12       4   \n",
       "4  Ink helps drive democracy in Asia       0           37       5   \n",
       "\n",
       "                                                                                                                                                                      Sent_tokens  \\\n",
       "0  [kyrgyz, republic, small, mountainous, state, former, soviet, republic, using, invisible, ink, ultraviolet, reader, country, election, part, drive, prevent, multiple, voting]   \n",
       "1                                                                                      [new, technology, causing, worry, guarded, optimism, among, different, sector, population]   \n",
       "2               [effort, live, reputation, island, democracy, kyrgyz, president, askar, akaev, pushed, law, requiring, use, ink, upcoming, parliamentary, presidential, election]   \n",
       "3                                                                                                                   [us, government, agreed, fund, expense, associated, decision]   \n",
       "4                [kyrgyz, republic, seen, many, expert, backsliding, high, point, reached, mid, hastily, pushed, referendum, reducing, legislative, branch, one, chamber, deputy]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             4.0               0.125  \n",
       "1             0.0                   0  \n",
       "2             4.0            0.105263  \n",
       "3             0.0                   0  \n",
       "4             2.0           0.0540541  "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maternity pay for new mothers is to rise by 1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>[maternity, pay, new, mother, rise, part, new, proposal, announced, trade, industry, secretary, patricia, hewitt]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It would mean paid leave would be increased to nine months by 2007, Ms Hewitt told GMTV's Sunday programme</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, mean, paid, leave, would, increased, nine, month, ms, hewitt, told, gmtv, sunday, programme]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other plans include letting maternity pay be given to fathers and extending rights to parents of older children</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>[plan, include, letting, maternity, pay, given, father, extending, right, parent, older, child]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Tories dismissed the maternity pay plan as desperate, while the Liberal Democrats said it was misdirected</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>[tories, dismissed, maternity, pay, plan, desperate, liberal, democrats, said, misdirected]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ms Hewitt said: We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>[ms, hewitt, said, already, doubled, length, maternity, pay, week, elected, already, taken, week]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Sentence  \\\n",
       "0          Maternity pay for new mothers is to rise by 1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt   \n",
       "1                                          It would mean paid leave would be increased to nine months by 2007, Ms Hewitt told GMTV's Sunday programme   \n",
       "2                                     Other plans include letting maternity pay be given to fathers and extending rights to parents of older children   \n",
       "3                                       The Tories dismissed the maternity pay plan as desperate, while the Liberal Democrats said it was misdirected   \n",
       "4  Ms Hewitt said: We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks   \n",
       "\n",
       "                             Title Summary Sentence_len Sent_no  \\\n",
       "0  Labour plans maternity pay rise       0           24       1   \n",
       "1  Labour plans maternity pay rise       0           19       2   \n",
       "2  Labour plans maternity pay rise       1           18       3   \n",
       "3  Labour plans maternity pay rise       1           17       4   \n",
       "4  Labour plans maternity pay rise       1           29       5   \n",
       "\n",
       "                                                                                                         Sent_tokens  \\\n",
       "0  [maternity, pay, new, mother, rise, part, new, proposal, announced, trade, industry, secretary, patricia, hewitt]   \n",
       "1               [would, mean, paid, leave, would, increased, nine, month, ms, hewitt, told, gmtv, sunday, programme]   \n",
       "2                    [plan, include, letting, maternity, pay, given, father, extending, right, parent, older, child]   \n",
       "3                        [tories, dismissed, maternity, pay, plan, desperate, liberal, democrats, said, misdirected]   \n",
       "4                  [ms, hewitt, said, already, doubled, length, maternity, pay, week, elected, already, taken, week]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             5.0            0.208333  \n",
       "1             3.0            0.157895  \n",
       "2             3.0            0.166667  \n",
       "3             4.0            0.235294  \n",
       "4             3.0            0.103448  "
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_politics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (600m) for the three months to December, from $639m year-earlier</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>[quarterly, profit, us, medium, giant, timewarner, jumped, bn, three, month, december, year, earlier]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>[firm, one, biggest, investor, google, benefited, sale, high, speed, internet, connection, higher, advert, sale]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>[timewarner, said, fourth, quarter, sale, rose, bn, bn]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>[profit, buoyed, one, gain, offset, profit, dip, warner, bros, le, user, aol]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time Warner said on Friday that it now owns 8% of search-engine Google</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>[time, warner, said, friday, owns, search, engine, google]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         Sentence  \\\n",
       "0         Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (600m) for the three months to December, from $639m year-earlier   \n",
       "1  The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales   \n",
       "2                                                                            TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn   \n",
       "3                                       Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL   \n",
       "4                                                                          Time Warner said on Friday that it now owns 8% of search-engine Google   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Ad sales boost Time Warner profit       0           21       1   \n",
       "1  Ad sales boost Time Warner profit       0           23       2   \n",
       "2  Ad sales boost Time Warner profit       0           11       3   \n",
       "3  Ad sales boost Time Warner profit       1           20       4   \n",
       "4  Ad sales boost Time Warner profit       0           13       5   \n",
       "\n",
       "                                                                                                        Sent_tokens  \\\n",
       "0             [quarterly, profit, us, medium, giant, timewarner, jumped, bn, three, month, december, year, earlier]   \n",
       "1  [firm, one, biggest, investor, google, benefited, sale, high, speed, internet, connection, higher, advert, sale]   \n",
       "2                                                           [timewarner, said, fourth, quarter, sale, rose, bn, bn]   \n",
       "3                                     [profit, buoyed, one, gain, offset, profit, dip, warner, bros, le, user, aol]   \n",
       "4                                                        [time, warner, said, friday, owns, search, engine, google]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             4.0            0.190476  \n",
       "1             3.0            0.130435  \n",
       "2             6.0            0.545455  \n",
       "3             3.0                0.15  \n",
       "4             1.0           0.0769231  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Christmas tree that can receive text messages has been unveiled at London's Tate Britain art gallery</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[christmas, tree, receive, text, message, unveiled, london, tate, britain, art, gallery]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[spruce, antenna, receive, bluetooth, text, sent, visitor, tate]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The messages will be unwrapped by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>[message, unwrapped, sculptor, richard, wentworth, responsible, decorating, tree, broken, plate, light, bulb]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is the 17th year that the gallery has invited an artist to dress their Christmas tree</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>[th, year, gallery, invited, artist, dress, christmas, tree]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artists who have decorated the Tate tree in previous years include Tracey Emin in 2002</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>[artists, decorated, tate, tree, previous, year, include, tracey, emin]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      Sentence  \\\n",
       "0                                       A Christmas tree that can receive text messages has been unveiled at London's Tate Britain art gallery   \n",
       "1                                                     The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate   \n",
       "2  The messages will be unwrapped by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs   \n",
       "3                                                     It is the 17th year that the gallery has invited an artist to dress their Christmas tree   \n",
       "4                                                       Artists who have decorated the Tate tree in previous years include Tracey Emin in 2002   \n",
       "\n",
       "                              Title Summary Sentence_len Sent_no  \\\n",
       "0  Gallery unveils interactive tree       1           17       1   \n",
       "1  Gallery unveils interactive tree       1           16       2   \n",
       "2  Gallery unveils interactive tree       1           22       3   \n",
       "3  Gallery unveils interactive tree       1           17       4   \n",
       "4  Gallery unveils interactive tree       0           15       5   \n",
       "\n",
       "                                                                                                     Sent_tokens  \\\n",
       "0                       [christmas, tree, receive, text, message, unveiled, london, tate, britain, art, gallery]   \n",
       "1                                               [spruce, antenna, receive, bluetooth, text, sent, visitor, tate]   \n",
       "2  [message, unwrapped, sculptor, richard, wentworth, responsible, decorating, tree, broken, plate, light, bulb]   \n",
       "3                                                   [th, year, gallery, invited, artist, dress, christmas, tree]   \n",
       "4                                        [artists, decorated, tate, tree, previous, year, include, tracey, emin]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  \n",
       "0             5.0            0.294118  \n",
       "1             3.0              0.1875  \n",
       "2             5.0            0.227273  \n",
       "3             3.0            0.176471  \n",
       "4             2.0            0.133333  "
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_entertainment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to normalize sentnece length within a file | to be used after the main_data is created \n",
    "def normal_sen_length(x,title,dataframe):   \n",
    "    high = max(dataframe.groupby('Title').get_group(title)['Sentence_len'])\n",
    "    return (x/high)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to score a sentence on the basis of its position in the text  WORKS \n",
    "def sent_pos(x,title,sent_count_dict):\n",
    "    \n",
    "    no_sent = sent_count_dict[title]\n",
    "    mid = no_sent/2 + 0.3\n",
    "    if (x == 1) or (x == no_sent):   # if the sentence is first or last return 1 \n",
    "        return(1)\n",
    "    else : \n",
    "        if x < mid:\n",
    "            return(np.cos((1-x)*((1/no_sent)-mid)))\n",
    "            \n",
    "        elif x> mid:\n",
    "            return(np.cos((no_sent-x)*((1/11)-mid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dictionary with title as key and the top15 nouns as items  \n",
    "# To be used to find no of top nouns present in each sentence \n",
    "top_noun_dict = {} \n",
    "def top_nouns(token_dict):\n",
    "           \n",
    "    for i in token_dict.keys():\n",
    "        \n",
    "        pos = nltk.pos_tag(token_dict[i])\n",
    "        count=0\n",
    "        noun_list=[]\n",
    "        for a,b in pos:\n",
    "            if (b==\"NN\") or (b==\"NNS\") or (b==\"NNP\") or (b==\"NNPS\"):\n",
    "                count+=1\n",
    "                noun_list.append(a)\n",
    "        top_15 = list(pd.Series(noun_list).value_counts().keys()[:15])    \n",
    "            \n",
    "        top_noun_dict[i] = top_15 \n",
    "    return(top_noun_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_noun_dict= top_nouns(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names=variable_names.append(pd.DataFrame({'Name':['top_noun_dict'],'info':'Dict file- key= Title, values = Top 15 noun for each title'}),\n",
    "                                    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them_dict</td>\n",
       "      <td>Dict file: key = title ,values = Thematic words for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token_dict</td>\n",
       "      <td>Dict file: key = title ,values = Tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word_count_dict</td>\n",
       "      <td>Dict file: key = title ,values = Total no of tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>main_data_sport</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>main_data_tech</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>main_data_politics</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>main_data_business</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>main_data_entertainment</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sent_count_dict_sport</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sent_count_dict_tech</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sent_count_dict_politics</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sent_count_dict_business</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sent_count_dict_entertainment</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>top_noun_dict</td>\n",
       "      <td>Dict file- key= Title, values = Top 15 noun for each title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                  sentence_sport   \n",
       "1                   sentence_tech   \n",
       "2               sentence_politics   \n",
       "3               sentence_business   \n",
       "4          sentence_entertainment   \n",
       "5              summary_dict_sport   \n",
       "6               summary_dict_tech   \n",
       "7           summary_dict_politics   \n",
       "8           summary_dict_business   \n",
       "9      summary_dict_entertainment   \n",
       "10                      them_dict   \n",
       "11                     token_dict   \n",
       "12                word_count_dict   \n",
       "13                main_data_sport   \n",
       "14                 main_data_tech   \n",
       "15             main_data_politics   \n",
       "16             main_data_business   \n",
       "17        main_data_entertainment   \n",
       "18          sent_count_dict_sport   \n",
       "19           sent_count_dict_tech   \n",
       "20       sent_count_dict_politics   \n",
       "21       sent_count_dict_business   \n",
       "22  sent_count_dict_entertainment   \n",
       "23                  top_noun_dict   \n",
       "\n",
       "                                                                                           info  \n",
       "0                                                                                   Corpus file  \n",
       "1                                                                                   Corpus file  \n",
       "2                                                                                   Corpus file  \n",
       "3                                                                                   Corpus file  \n",
       "4                                                                                   Corpus file  \n",
       "5                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "10      Dict file: key = title ,values = Thematic words for each title(includes all categories)  \n",
       "11              Dict file: key = title ,values = Tokens for each title(includes all categories)  \n",
       "12  Dict file: key = title ,values = Total no of tokens for each title(includes all categories)  \n",
       "13                                                                 Data Frame for each category  \n",
       "14                                                                 Data Frame for each category  \n",
       "15                                                                 Data Frame for each category  \n",
       "16                                                                 Data Frame for each category  \n",
       "17                                                                 Data Frame for each category  \n",
       "18                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "19                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "20                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "21                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "22                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "23                                   Dict file- key= Title, values = Top 15 noun for each title  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_noun_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from the base data frame  # works \n",
    "# Doesn't return anything ,modifies the data frame used as argument\n",
    "def features_(dataframe,sent_count_dict):\n",
    "    \n",
    "    dataframe['Norm_sent_length'] =0\n",
    "    dataframe['Sent_pos_norm'] = 0 \n",
    "    dataframe['No_top_noun_norm'] = 0 \n",
    "    \n",
    "    \n",
    "    for i in dataframe.index:\n",
    "        title = dataframe['Title'][i]\n",
    "        \n",
    "        dataframe.loc[i,'Norm_sent_length'] = normal_sen_length(dataframe.Sentence_len[i],title,dataframe)  \n",
    "                                                                           \n",
    "        dataframe.loc[i,'Sent_pos_norm'] = sent_pos(dataframe.Sent_no[i],title,sent_count_dict)\n",
    "        \n",
    "        nn = [a for a in dataframe.loc[i,'Sent_tokens'] if a in top_noun_dict[dataframe.loc[i,'Title']]]  \n",
    "        dataframe.loc[i,'No_top_noun_norm'] = len(nn)/dataframe.loc[i,'Sentence_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_(main_data_sport,sent_count_dict_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_(main_data_tech,sent_count_dict_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_(main_data_politics,sent_count_dict_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_(main_data_business,sent_count_dict_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_(main_data_entertainment,sent_count_dict_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "      <th>Norm_sent_length</th>\n",
       "      <th>Sent_pos_norm</th>\n",
       "      <th>No_top_noun_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>[british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title</td>\n",
       "      <td>Claxton hunting first major medal</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>[year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839684</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              Sentence  \\\n",
       "0                 British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid   \n",
       "1  The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Claxton hunting first major medal       1           21       1   \n",
       "1  Claxton hunting first major medal       0           26       2   \n",
       "\n",
       "                                                                                                                     Sent_tokens  \\\n",
       "0  [british, hurdler, sarah, claxton, confident, win, first, major, medal, next, month, european, indoor, championships, madrid]   \n",
       "1            [year, old, already, smashed, british, record, hurdle, twice, season, setting, new, mark, second, win, aaas, title]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  Norm_sent_length  Sent_pos_norm  \\\n",
       "0             5.0            0.238095          0.807692       1.000000   \n",
       "1             4.0            0.153846          1.000000       0.839684   \n",
       "\n",
       "   No_top_noun_norm  \n",
       "0          0.190476  \n",
       "1          0.269231  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2 VEC model implementation - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100   # no of dimensions\n",
    "min_word_count = 10  # criteria \n",
    "context_size = 3     # window size\n",
    "downsampling = 1e-3\n",
    "seed=1000            # random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create doument level embeddings from word embeddings by average- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\") \n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.build_vocab(token_dict.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 5736\n",
      "Documents: 2096\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec vocabulary length:', len(word2vec.wv.vocab))\n",
    "print(\"Documents:\", word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913012, 2232595)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.train(token_dict.values(), total_examples=word2vec.corpus_count, epochs=word2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# get document level embeddings to be used in modeling \n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=main_data_sport['Sent_tokens'], model=word2vec,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "vector_df_sport= pd.DataFrame(w2v_feature_array)  # dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# get document level embeddings to be used in modeling \n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=main_data_tech['Sent_tokens'], model=word2vec,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "vector_df_tech= pd.DataFrame(w2v_feature_array)  # dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.515983</td>\n",
       "      <td>0.084560</td>\n",
       "      <td>0.129212</td>\n",
       "      <td>-0.173141</td>\n",
       "      <td>0.035930</td>\n",
       "      <td>0.213185</td>\n",
       "      <td>-0.030054</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.012544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122754</td>\n",
       "      <td>0.123235</td>\n",
       "      <td>-0.092908</td>\n",
       "      <td>0.530242</td>\n",
       "      <td>0.131693</td>\n",
       "      <td>0.131590</td>\n",
       "      <td>0.111715</td>\n",
       "      <td>-0.027947</td>\n",
       "      <td>-0.016871</td>\n",
       "      <td>0.127335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.464416</td>\n",
       "      <td>0.229689</td>\n",
       "      <td>0.109913</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>0.086589</td>\n",
       "      <td>0.183555</td>\n",
       "      <td>-0.012466</td>\n",
       "      <td>-0.070960</td>\n",
       "      <td>0.039257</td>\n",
       "      <td>0.030102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240201</td>\n",
       "      <td>0.091528</td>\n",
       "      <td>-0.096516</td>\n",
       "      <td>0.433443</td>\n",
       "      <td>0.118160</td>\n",
       "      <td>0.147319</td>\n",
       "      <td>0.167864</td>\n",
       "      <td>-0.097364</td>\n",
       "      <td>0.092007</td>\n",
       "      <td>0.118312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.317820</td>\n",
       "      <td>0.181834</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>-0.033570</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>0.429534</td>\n",
       "      <td>-0.181011</td>\n",
       "      <td>-0.166992</td>\n",
       "      <td>0.164867</td>\n",
       "      <td>-0.020181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033546</td>\n",
       "      <td>-0.017858</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.367749</td>\n",
       "      <td>0.068851</td>\n",
       "      <td>0.134343</td>\n",
       "      <td>0.117725</td>\n",
       "      <td>0.154078</td>\n",
       "      <td>0.192073</td>\n",
       "      <td>0.007099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.392956</td>\n",
       "      <td>0.273047</td>\n",
       "      <td>0.181701</td>\n",
       "      <td>-0.157584</td>\n",
       "      <td>-0.088705</td>\n",
       "      <td>0.147133</td>\n",
       "      <td>-0.113166</td>\n",
       "      <td>-0.041116</td>\n",
       "      <td>0.091648</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225280</td>\n",
       "      <td>0.029737</td>\n",
       "      <td>-0.188004</td>\n",
       "      <td>0.434711</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>0.302131</td>\n",
       "      <td>0.126363</td>\n",
       "      <td>0.061627</td>\n",
       "      <td>0.225917</td>\n",
       "      <td>-0.101479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.431828</td>\n",
       "      <td>0.294369</td>\n",
       "      <td>0.190404</td>\n",
       "      <td>-0.224207</td>\n",
       "      <td>-0.017567</td>\n",
       "      <td>0.314426</td>\n",
       "      <td>-0.167363</td>\n",
       "      <td>-0.243903</td>\n",
       "      <td>0.079288</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151252</td>\n",
       "      <td>0.086315</td>\n",
       "      <td>-0.062254</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.093205</td>\n",
       "      <td>0.164247</td>\n",
       "      <td>0.183067</td>\n",
       "      <td>0.167401</td>\n",
       "      <td>0.252012</td>\n",
       "      <td>0.034752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.515983  0.084560  0.129212 -0.173141  0.035930  0.213185 -0.030054   \n",
       "1 -0.464416  0.229689  0.109913 -0.221998  0.086589  0.183555 -0.012466   \n",
       "2 -0.317820  0.181834  0.333400 -0.033570  0.127333  0.429534 -0.181011   \n",
       "3 -0.392956  0.273047  0.181701 -0.157584 -0.088705  0.147133 -0.113166   \n",
       "4 -0.431828  0.294369  0.190404 -0.224207 -0.017567  0.314426 -0.167363   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -0.029890  0.037256  0.012544  ... -0.122754  0.123235 -0.092908  0.530242   \n",
       "1 -0.070960  0.039257  0.030102  ... -0.240201  0.091528 -0.096516  0.433443   \n",
       "2 -0.166992  0.164867 -0.020181  ... -0.033546 -0.017858  0.017572  0.367749   \n",
       "3 -0.041116  0.091648  0.062943  ... -0.225280  0.029737 -0.188004  0.434711   \n",
       "4 -0.243903  0.079288 -0.002162  ... -0.151252  0.086315 -0.062254  0.349998   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.131693  0.131590  0.111715 -0.027947 -0.016871  0.127335  \n",
       "1  0.118160  0.147319  0.167864 -0.097364  0.092007  0.118312  \n",
       "2  0.068851  0.134343  0.117725  0.154078  0.192073  0.007099  \n",
       "3  0.084066  0.302131  0.126363  0.061627  0.225917 -0.101479  \n",
       "4  0.093205  0.164247  0.183067  0.167401  0.252012  0.034752  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df_sport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.390092</td>\n",
       "      <td>0.092528</td>\n",
       "      <td>0.289281</td>\n",
       "      <td>-0.244863</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.183847</td>\n",
       "      <td>-0.192284</td>\n",
       "      <td>-0.026346</td>\n",
       "      <td>0.154612</td>\n",
       "      <td>-0.053360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145950</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>-0.036040</td>\n",
       "      <td>0.289099</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>0.050947</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>-0.029082</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>0.200983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.469936</td>\n",
       "      <td>0.142580</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>-0.314490</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.208672</td>\n",
       "      <td>-0.189448</td>\n",
       "      <td>-0.069037</td>\n",
       "      <td>0.219386</td>\n",
       "      <td>-0.156101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058039</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>-0.061966</td>\n",
       "      <td>0.245402</td>\n",
       "      <td>0.061116</td>\n",
       "      <td>-0.005235</td>\n",
       "      <td>0.245714</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.063414</td>\n",
       "      <td>0.196080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.413447</td>\n",
       "      <td>0.104805</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>-0.198381</td>\n",
       "      <td>-0.002469</td>\n",
       "      <td>0.243369</td>\n",
       "      <td>-0.154968</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.156119</td>\n",
       "      <td>-0.038166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192757</td>\n",
       "      <td>0.069846</td>\n",
       "      <td>-0.044658</td>\n",
       "      <td>0.278496</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>0.038363</td>\n",
       "      <td>-0.015539</td>\n",
       "      <td>0.107614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.308551</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.295427</td>\n",
       "      <td>-0.315132</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>0.328438</td>\n",
       "      <td>-0.216648</td>\n",
       "      <td>0.029682</td>\n",
       "      <td>0.159675</td>\n",
       "      <td>-0.025479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084597</td>\n",
       "      <td>-0.061116</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.208829</td>\n",
       "      <td>0.089060</td>\n",
       "      <td>-0.025154</td>\n",
       "      <td>0.220348</td>\n",
       "      <td>-0.013235</td>\n",
       "      <td>-0.124126</td>\n",
       "      <td>0.212590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.452029</td>\n",
       "      <td>0.128550</td>\n",
       "      <td>0.208731</td>\n",
       "      <td>-0.274361</td>\n",
       "      <td>0.023974</td>\n",
       "      <td>0.271625</td>\n",
       "      <td>-0.172091</td>\n",
       "      <td>-0.108232</td>\n",
       "      <td>0.154202</td>\n",
       "      <td>0.013482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184657</td>\n",
       "      <td>0.117106</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>0.334764</td>\n",
       "      <td>0.075032</td>\n",
       "      <td>0.066526</td>\n",
       "      <td>0.169607</td>\n",
       "      <td>-0.044115</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.174997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.390092  0.092528  0.289281 -0.244863 -0.014091  0.183847 -0.192284   \n",
       "1 -0.469936  0.142580  0.338346 -0.314490  0.022308  0.208672 -0.189448   \n",
       "2 -0.413447  0.104805  0.224787 -0.198381 -0.002469  0.243369 -0.154968   \n",
       "3 -0.308551  0.053362  0.295427 -0.315132 -0.011905  0.328438 -0.216648   \n",
       "4 -0.452029  0.128550  0.208731 -0.274361  0.023974  0.271625 -0.172091   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -0.026346  0.154612 -0.053360  ... -0.145950  0.034217 -0.036040  0.289099   \n",
       "1 -0.069037  0.219386 -0.156101  ... -0.058039  0.003372 -0.061966  0.245402   \n",
       "2  0.012168  0.156119 -0.038166  ... -0.192757  0.069846 -0.044658  0.278496   \n",
       "3  0.029682  0.159675 -0.025479  ... -0.084597 -0.061116  0.011061  0.208829   \n",
       "4 -0.108232  0.154202  0.013482  ... -0.184657  0.117106 -0.042726  0.334764   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.057072  0.050947  0.178662 -0.029082 -0.019119  0.200983  \n",
       "1  0.061116 -0.005235  0.245714  0.022515  0.063414  0.196080  \n",
       "2  0.016530  0.113235  0.099849  0.038363 -0.015539  0.107614  \n",
       "3  0.089060 -0.025154  0.220348 -0.013235 -0.124126  0.212590  \n",
       "4  0.075032  0.066526  0.169607 -0.044115  0.023457  0.174997  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df_tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# get document level embeddings to be used in modeling \n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=main_data_politics['Sent_tokens'], model=word2vec,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "vector_df_politics= pd.DataFrame(w2v_feature_array)  # dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# get document level embeddings to be used in modeling \n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=main_data_business['Sent_tokens'], model=word2vec,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "vector_df_business= pd.DataFrame(w2v_feature_array)  # dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# get document level embeddings to be used in modeling \n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=main_data_entertainment['Sent_tokens'], model=word2vec,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "vector_df_entertainment= pd.DataFrame(w2v_feature_array)  # dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8653, 100)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8653, 11)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them_dict</td>\n",
       "      <td>Dict file: key = title ,values = Thematic words for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token_dict</td>\n",
       "      <td>Dict file: key = title ,values = Tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word_count_dict</td>\n",
       "      <td>Dict file: key = title ,values = Total no of tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>main_data_sport</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>main_data_tech</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>main_data_politics</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>main_data_business</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>main_data_entertainment</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sent_count_dict_sport</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sent_count_dict_tech</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sent_count_dict_politics</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sent_count_dict_business</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sent_count_dict_entertainment</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>top_noun_dict</td>\n",
       "      <td>Dict file- key= Title, values = Top 15 noun for each title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                  sentence_sport   \n",
       "1                   sentence_tech   \n",
       "2               sentence_politics   \n",
       "3               sentence_business   \n",
       "4          sentence_entertainment   \n",
       "5              summary_dict_sport   \n",
       "6               summary_dict_tech   \n",
       "7           summary_dict_politics   \n",
       "8           summary_dict_business   \n",
       "9      summary_dict_entertainment   \n",
       "10                      them_dict   \n",
       "11                     token_dict   \n",
       "12                word_count_dict   \n",
       "13                main_data_sport   \n",
       "14                 main_data_tech   \n",
       "15             main_data_politics   \n",
       "16             main_data_business   \n",
       "17        main_data_entertainment   \n",
       "18          sent_count_dict_sport   \n",
       "19           sent_count_dict_tech   \n",
       "20       sent_count_dict_politics   \n",
       "21       sent_count_dict_business   \n",
       "22  sent_count_dict_entertainment   \n",
       "23                  top_noun_dict   \n",
       "\n",
       "                                                                                           info  \n",
       "0                                                                                   Corpus file  \n",
       "1                                                                                   Corpus file  \n",
       "2                                                                                   Corpus file  \n",
       "3                                                                                   Corpus file  \n",
       "4                                                                                   Corpus file  \n",
       "5                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "10      Dict file: key = title ,values = Thematic words for each title(includes all categories)  \n",
       "11              Dict file: key = title ,values = Tokens for each title(includes all categories)  \n",
       "12  Dict file: key = title ,values = Total no of tokens for each title(includes all categories)  \n",
       "13                                                                 Data Frame for each category  \n",
       "14                                                                 Data Frame for each category  \n",
       "15                                                                 Data Frame for each category  \n",
       "16                                                                 Data Frame for each category  \n",
       "17                                                                 Data Frame for each category  \n",
       "18                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "19                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "20                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "21                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "22                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "23                                   Dict file- key= Title, values = Top 15 noun for each title  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6142, 100)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df_entertainment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6142, 11)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_entertainment.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = variable_names.append(pd.DataFrame({\"Name\":['vector_df_sport','vector_df_tech',\n",
    "                                                             'vector_df_politics','vector_df_business',\n",
    "                                                             'vector_df_entertainment'],\n",
    "                                                     'info':\"DataFrame of sentence vectors category wise\"}),ignore_index=True)\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_sport</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_tech</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_politics</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_business</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_entertainment</td>\n",
       "      <td>Corpus file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary_dict_sport</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summary_dict_tech</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summary_dict_politics</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary_dict_business</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_dict_entertainment</td>\n",
       "      <td>Dictionary file- Key = file_no,values=Summary sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>them_dict</td>\n",
       "      <td>Dict file: key = title ,values = Thematic words for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token_dict</td>\n",
       "      <td>Dict file: key = title ,values = Tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word_count_dict</td>\n",
       "      <td>Dict file: key = title ,values = Total no of tokens for each title(includes all categories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>main_data_sport</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>main_data_tech</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>main_data_politics</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>main_data_business</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>main_data_entertainment</td>\n",
       "      <td>Data Frame for each category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sent_count_dict_sport</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sent_count_dict_tech</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sent_count_dict_politics</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sent_count_dict_business</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sent_count_dict_entertainment</td>\n",
       "      <td>Dict for each category- key= Title , values =No of sentences in each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>top_noun_dict</td>\n",
       "      <td>Dict file- key= Title, values = Top 15 noun for each title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vector_df_sport</td>\n",
       "      <td>DataFrame of sentence vectors category wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vector_df_tech</td>\n",
       "      <td>DataFrame of sentence vectors category wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>vector_df_politics</td>\n",
       "      <td>DataFrame of sentence vectors category wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>vector_df_business</td>\n",
       "      <td>DataFrame of sentence vectors category wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>vector_df_entertainment</td>\n",
       "      <td>DataFrame of sentence vectors category wise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                  sentence_sport   \n",
       "1                   sentence_tech   \n",
       "2               sentence_politics   \n",
       "3               sentence_business   \n",
       "4          sentence_entertainment   \n",
       "5              summary_dict_sport   \n",
       "6               summary_dict_tech   \n",
       "7           summary_dict_politics   \n",
       "8           summary_dict_business   \n",
       "9      summary_dict_entertainment   \n",
       "10                      them_dict   \n",
       "11                     token_dict   \n",
       "12                word_count_dict   \n",
       "13                main_data_sport   \n",
       "14                 main_data_tech   \n",
       "15             main_data_politics   \n",
       "16             main_data_business   \n",
       "17        main_data_entertainment   \n",
       "18          sent_count_dict_sport   \n",
       "19           sent_count_dict_tech   \n",
       "20       sent_count_dict_politics   \n",
       "21       sent_count_dict_business   \n",
       "22  sent_count_dict_entertainment   \n",
       "23                  top_noun_dict   \n",
       "24                vector_df_sport   \n",
       "25                 vector_df_tech   \n",
       "26             vector_df_politics   \n",
       "27             vector_df_business   \n",
       "28        vector_df_entertainment   \n",
       "\n",
       "                                                                                           info  \n",
       "0                                                                                   Corpus file  \n",
       "1                                                                                   Corpus file  \n",
       "2                                                                                   Corpus file  \n",
       "3                                                                                   Corpus file  \n",
       "4                                                                                   Corpus file  \n",
       "5                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "6                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "7                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "8                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "9                                       Dictionary file- Key = file_no,values=Summary sentences  \n",
       "10      Dict file: key = title ,values = Thematic words for each title(includes all categories)  \n",
       "11              Dict file: key = title ,values = Tokens for each title(includes all categories)  \n",
       "12  Dict file: key = title ,values = Total no of tokens for each title(includes all categories)  \n",
       "13                                                                 Data Frame for each category  \n",
       "14                                                                 Data Frame for each category  \n",
       "15                                                                 Data Frame for each category  \n",
       "16                                                                 Data Frame for each category  \n",
       "17                                                                 Data Frame for each category  \n",
       "18                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "19                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "20                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "21                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "22                   Dict for each category- key= Title , values =No of sentences in each title  \n",
       "23                                   Dict file- key= Title, values = Top 15 noun for each title  \n",
       "24                                                  DataFrame of sentence vectors category wise  \n",
       "25                                                  DataFrame of sentence vectors category wise  \n",
       "26                                                  DataFrame of sentence vectors category wise  \n",
       "27                                                  DataFrame of sentence vectors category wise  \n",
       "28                                                  DataFrame of sentence vectors category wise  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging data frame and respective vector data frame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_sport = main_data_sport.merge(vector_df_sport,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8653, 111)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_sport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_tech = main_data_tech.merge(vector_df_tech ,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_politics = main_data_politics.merge(vector_df_politics ,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_business = main_data_business.merge(vector_df_business ,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_entertainment = main_data_entertainment.merge(vector_df_entertainment,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "      <th>Norm_sent_length</th>\n",
       "      <th>Sent_pos_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting</td>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>[kyrgyz, republic, small, mountainous, state, former, soviet, republic, using, invisible, ink, ultraviolet, reader, country, election, part, drive, prevent, multiple, voting]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14595</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>-0.03604</td>\n",
       "      <td>0.289099</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>0.050947</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>-0.029082</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>0.200983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                 Sentence  \\\n",
       "0  The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting   \n",
       "\n",
       "                               Title Summary Sentence_len Sent_no  \\\n",
       "0  Ink helps drive democracy in Asia       1           32       1   \n",
       "\n",
       "                                                                                                                                                                      Sent_tokens  \\\n",
       "0  [kyrgyz, republic, small, mountainous, state, former, soviet, republic, using, invisible, ink, ultraviolet, reader, country, election, part, drive, prevent, multiple, voting]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  Norm_sent_length  Sent_pos_norm  ...  \\\n",
       "0             4.0               0.125          0.680851            1.0  ...   \n",
       "\n",
       "        90        91       92        93        94        95        96  \\\n",
       "0 -0.14595  0.034217 -0.03604  0.289099  0.057072  0.050947  0.178662   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.029082 -0.019119  0.200983  \n",
       "\n",
       "[1 rows x 111 columns]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_tech.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "      <th>Norm_sent_length</th>\n",
       "      <th>Sent_pos_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maternity pay for new mothers is to rise by 1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt</td>\n",
       "      <td>Labour plans maternity pay rise</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>[maternity, pay, new, mother, rise, part, new, proposal, announced, trade, industry, secretary, patricia, hewitt]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064972</td>\n",
       "      <td>-0.019273</td>\n",
       "      <td>-0.016031</td>\n",
       "      <td>0.244475</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.144151</td>\n",
       "      <td>-0.104846</td>\n",
       "      <td>-0.017387</td>\n",
       "      <td>0.103017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     Sentence  \\\n",
       "0  Maternity pay for new mothers is to rise by 1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt   \n",
       "\n",
       "                             Title Summary Sentence_len Sent_no  \\\n",
       "0  Labour plans maternity pay rise       0           24       1   \n",
       "\n",
       "                                                                                                         Sent_tokens  \\\n",
       "0  [maternity, pay, new, mother, rise, part, new, proposal, announced, trade, industry, secretary, patricia, hewitt]   \n",
       "\n",
       "   Thematic_count Thematic_count_norm  Norm_sent_length  Sent_pos_norm  ...  \\\n",
       "0             5.0            0.208333               0.6            1.0  ...   \n",
       "\n",
       "         90        91        92        93        94       95        96  \\\n",
       "0 -0.064972 -0.019273 -0.016031  0.244475  0.120344  0.00225  0.144151   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.104846 -0.017387  0.103017  \n",
       "\n",
       "[1 rows x 111 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_politics.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from vectors - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find centroid vector for each title, and vector for each title\n",
    "# works \n",
    "my_dict = dict({})\n",
    "for idx, key in enumerate(word2vec.wv.vocab):\n",
    "    my_dict[key] = word2vec.wv[key]\n",
    "\n",
    "centroid_dict={}\n",
    "title_vector_dict={}\n",
    "def centroid_vector(dataframe):\n",
    "           \n",
    "    for i in dataframe.Title.unique():\n",
    "        centroid_dict [i] = dataframe.groupby('Title').get_group(i)[range(100)].mean(axis=0)  # centroid of each title\n",
    "        \n",
    "        vectors = np.zeros(100)\n",
    "        token_title = sent_wise_token(i)\n",
    "        for k in token_title:\n",
    "            if k in my_dict.keys():\n",
    "                vectors = vectors + my_dict[k]\n",
    "            else: \n",
    "                vectors = vectors + np.zeros(100)\n",
    "        title_vector_dict[i] = vectors/(len(token_title))        # vector of title \n",
    "       \n",
    "    \n",
    "    return (centroid_dict,title_vector_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict,title_vector_dict = centroid_vector(main_data_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict,title_vector_dict = centroid_vector(main_data_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict,title_vector_dict = centroid_vector(main_data_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict,title_vector_dict = centroid_vector(main_data_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict,title_vector_dict = centroid_vector(main_data_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(centroid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_func(a,b):  # to be used in calculating sent to title and sent to centroid similarity\n",
    "    return (np.dot(a,b) / ( (np.dot(a,a) **.5) * (np.dot(b,b) ** .5) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to find cosine_similarity  WORKS \n",
    "\n",
    "def sent_simlilarity(dataframe):\n",
    "    \n",
    "    for i in dataframe.index:\n",
    "        \n",
    "        title = dataframe.loc[i,'Title']\n",
    "        \n",
    "        dataframe.loc[i,'sent2title_sim'] = cosine_similarity_func( title_vector_dict[title]\n",
    "                                                                   , dataframe.loc[i,range(100)])\n",
    "        \n",
    "        dataframe.loc[i,'sent2centroid_sim'] = cosine_similarity_func(centroid_dict[title]\n",
    "                                                                   , dataframe.loc[i,range(100)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        comparison_vectors = dataframe.groupby('Title').get_group(title)[range(100)]\n",
    "        \n",
    "        similar = cosine_similarity(comparison_vectors,comparison_vectors)\n",
    "        \n",
    "        sim_score = similar[dataframe.loc[i,'Sent_no']-1].sum() -1\n",
    "        dataframe.loc[i,'Sent_sim_score'] = sim_score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_simlilarity(main_data_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_simlilarity(main_data_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_simlilarity(main_data_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_simlilarity(main_data_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_simlilarity(main_data_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalize the sentnece similar scores WORKS \n",
    "\n",
    "def sent_similar_norm(dataframe):\n",
    "    \n",
    "    for i in dataframe.index:\n",
    "        title = dataframe['Title'][i]\n",
    "        \n",
    "        dataframe.loc[i,'sent_sim_norm'] = dataframe.loc[i,'Sent_sim_score']/max(dataframe.groupby('Title').\n",
    "                                                                                  get_group(title)['Sent_sim_score'])\n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_similar_norm(main_data_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_similar_norm(main_data_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_similar_norm(main_data_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_similar_norm(main_data_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_similar_norm(main_data_entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a consolidated dataframe- \n",
    "data_frame = main_data_sport.copy(deep=True)\n",
    "\n",
    "data_frame = data_frame.append([main_data_tech,main_data_politics,main_data_business,main_data_entertainment],\n",
    "                                ignore_index= True, sort=False)\n",
    "\n",
    "data_frame.reset_index(inplace=True,drop=True)\n",
    "data_frame =data_frame.fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40963, 115)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentence_len</th>\n",
       "      <th>Sent_no</th>\n",
       "      <th>Sent_tokens</th>\n",
       "      <th>Thematic_count</th>\n",
       "      <th>Thematic_count_norm</th>\n",
       "      <th>Norm_sent_length</th>\n",
       "      <th>Sent_pos_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>sent2title_sim</th>\n",
       "      <th>sent2centroid_sim</th>\n",
       "      <th>Sent_sim_score</th>\n",
       "      <th>sent_sim_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40961</th>\n",
       "      <td>He also produced the spin-off Angel series</td>\n",
       "      <td>Buffy creator joins Wonder Woman</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>[also, produced, spin, angel, series]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.144384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.120191</td>\n",
       "      <td>0.113684</td>\n",
       "      <td>-0.020410</td>\n",
       "      <td>0.044367</td>\n",
       "      <td>0.114018</td>\n",
       "      <td>0.923749</td>\n",
       "      <td>0.964078</td>\n",
       "      <td>12.728616</td>\n",
       "      <td>0.971712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40962</th>\n",
       "      <td>He is currently directing the film Serenity, based on his short-lived sci-fi series Firefly</td>\n",
       "      <td>Buffy creator joins Wonder Woman</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>[currently, directing, film, serenity, based, short, lived, sci, fi, series, firefly]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044269</td>\n",
       "      <td>0.090079</td>\n",
       "      <td>0.179665</td>\n",
       "      <td>-0.026092</td>\n",
       "      <td>0.062902</td>\n",
       "      <td>0.196293</td>\n",
       "      <td>0.872598</td>\n",
       "      <td>0.947429</td>\n",
       "      <td>12.473704</td>\n",
       "      <td>0.952251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          Sentence  \\\n",
       "40961                                                   He also produced the spin-off Angel series   \n",
       "40962  He is currently directing the film Serenity, based on his short-lived sci-fi series Firefly   \n",
       "\n",
       "                                  Title  Summary  Sentence_len  Sent_no  \\\n",
       "40961  Buffy creator joins Wonder Woman        0             7       14   \n",
       "40962  Buffy creator joins Wonder Woman        0            14       15   \n",
       "\n",
       "                                                                                 Sent_tokens  \\\n",
       "40961                                                  [also, produced, spin, angel, series]   \n",
       "40962  [currently, directing, film, serenity, based, short, lived, sci, fi, series, firefly]   \n",
       "\n",
       "       Thematic_count  Thematic_count_norm  Norm_sent_length  Sent_pos_norm  \\\n",
       "40961             2.0             0.285714          0.233333       0.144384   \n",
       "40962             2.0             0.142857          0.466667       1.000000   \n",
       "\n",
       "       ...        94        95        96        97        98        99  \\\n",
       "40961  ...  0.053481  0.120191  0.113684 -0.020410  0.044367  0.114018   \n",
       "40962  ...  0.044269  0.090079  0.179665 -0.026092  0.062902  0.196293   \n",
       "\n",
       "       sent2title_sim  sent2centroid_sim  Sent_sim_score  sent_sim_norm  \n",
       "40961        0.923749           0.964078       12.728616       0.971712  \n",
       "40962        0.872598           0.947429       12.473704       0.952251  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('Consolidated_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [i for i in main_data_sport.columns if i not in ['Sentence','Title','Sentence_len',\n",
    "                                                                 'Sent_no','Sent_tokens','Thematic_count'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with features for modelling- \n",
    "# Starting with sports data\n",
    "\n",
    "X = main_data_sport[model_features]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief description of the features of the consolidated dataset -  \n",
    "\n",
    "\n",
    "\n",
    "### 1. Sentence  - \n",
    "Each row of the dataset is an individual sentence present in the all the documents, across the categories. \n",
    "\n",
    "### 2. Title - \n",
    "The respective title of the document the sentence belongs to. Used for finding Sentence to title similarity scores and other operations \n",
    "\n",
    "### 3. Summary (Target Variable) -\n",
    "A binary variable with 1 meaning the sentence is present in the summary , 0 if not. \n",
    "\n",
    "### 4. Sentence_len -\n",
    "Number of words present in each sentence(row)\n",
    "\n",
    "### 5. Sent_no - \n",
    "The position of a sentence in the main document. Starting from 1 to the total number of sentences present in each document.\n",
    "\n",
    "### 6. Sent_tokens - \n",
    "Tokens for each sentence (excluding the stop words,punctuations,digits ). To be used for extracting syntactic featurs, and for generating vectorial representation of each sentence. \n",
    "\n",
    "### 7. Thematic_count - \n",
    "Thematic words are defined as the most frequently occuring words in a document (post the preprocessing step). For the current analysis , top 10 thematic words are chosen for each document. A thematic word count is obtained for each sentence, which is then further normalised by dividing it with the no of words in the sentence. \n",
    "\n",
    "### 8. Thematic_count_norm - \n",
    "Normalised thematic words count for each sentence. \n",
    "\n",
    "### 9. Norm_sent_length - \n",
    "Normalised sentence length . It is the Ratio of the length of a sentence to the maximum value of sentence length for that document (Title)\n",
    "\n",
    "### 10. Sent_pos_norm - \n",
    "A normalised feature which represents the position of a sentence with respect to the beginning or end of a document. \n",
    "The first and the last sentence are given a value of 1, rest values are as per the distance from first or last. \n",
    "\n",
    "### 11. No_top_noun - \n",
    "Normalised feature for number of occurence of top 15 most frequently occuring nouns in a sentence. \n",
    "\n",
    "###  12. 0 - 99\n",
    "These 100 features are the vector dimensions used for word/sentence embeddings . Used for calculating similarity scores and modelling. \n",
    "\n",
    "### 13. sent2title_sim - \n",
    "Cosine similarity score between each sentence and the respective document title. Range 0 - 1.\n",
    "\n",
    "### 14. sent2centroid_sim\n",
    "Cosine similarity between each sentence and the respective centroid vector of the document. Range 0 - 1. \n",
    "\n",
    "### 15. Sent_sim_score \n",
    "Summation of Cosine similarity score between a sentence and the remaining sentences of the document. \n",
    "\n",
    "### 16. sent_sim_norm - \n",
    "Normalised sentence to sentence similarity score. Ratio of Sent_sim_score to the maximum value of Sent_sim_score across the document(title)\n",
    "\n",
    "A total of 108 features were used in building the supervised classification model. They are - \n",
    "1. Summary (target variable)\n",
    "2. Thematic_count_norm\n",
    "3. Norm_sent_length\n",
    "4. Sent_pos_norm\n",
    "5. No_top_noun_norm\n",
    "6. sent2title_sim\n",
    "7. sent2centroid_sim\n",
    "8. sent_sim_norm\n",
    "9. 100 Vector Dimensions (0-99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary                 object\n",
       "Thematic_count_norm     object\n",
       "Norm_sent_length       float64\n",
       "Sent_pos_norm          float64\n",
       "No_top_noun_norm       float64\n",
       "0                      float64\n",
       "1                      float64\n",
       "2                      float64\n",
       "3                      float64\n",
       "4                      float64\n",
       "5                      float64\n",
       "6                      float64\n",
       "7                      float64\n",
       "8                      float64\n",
       "9                      float64\n",
       "10                     float64\n",
       "11                     float64\n",
       "12                     float64\n",
       "13                     float64\n",
       "14                     float64\n",
       "15                     float64\n",
       "16                     float64\n",
       "17                     float64\n",
       "18                     float64\n",
       "19                     float64\n",
       "20                     float64\n",
       "21                     float64\n",
       "22                     float64\n",
       "23                     float64\n",
       "24                     float64\n",
       "                        ...   \n",
       "74                     float64\n",
       "75                     float64\n",
       "76                     float64\n",
       "77                     float64\n",
       "78                     float64\n",
       "79                     float64\n",
       "80                     float64\n",
       "81                     float64\n",
       "82                     float64\n",
       "83                     float64\n",
       "84                     float64\n",
       "85                     float64\n",
       "86                     float64\n",
       "87                     float64\n",
       "88                     float64\n",
       "89                     float64\n",
       "90                     float64\n",
       "91                     float64\n",
       "92                     float64\n",
       "93                     float64\n",
       "94                     float64\n",
       "95                     float64\n",
       "96                     float64\n",
       "97                     float64\n",
       "98                     float64\n",
       "99                     float64\n",
       "sent2title_sim         float64\n",
       "sent2centroid_sim      float64\n",
       "Sent_sim_score         float64\n",
       "sent_sim_norm          float64\n",
       "Length: 109, dtype: object"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(main_data_tech[model_features],ignore_index= True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(main_data_politics[model_features],ignore_index= True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(main_data_business[model_features],ignore_index= True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.append(main_data_entertainment[model_features],ignore_index= True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=40963, step=1)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X.reset_index(inplace=True,drop=True)\n",
    "X = X.fillna(0)   # Filling the similarities with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40963, 108)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combined data frame with each row as a sentence\n",
    "X.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prparing the target variable -\n",
    "y = X['Summary']\n",
    "X.drop('Summary',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40963, 108), (40963,))"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    24923\n",
       "1.0    16040\n",
       "Name: Summary, dtype: int64"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.3,random_state = 100,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a baseline RandomForest model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=100,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on train set - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998953756015903"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on test set - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7733745626169746"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is evident from the train test accuracy scores that the model has clearly suffered from overfitting. \n",
    "#### Performing a Grid search to reduce the overfitting - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid- \n",
    "param_rf={'max_depth':[5,6,7,8],'min_samples_split':[5,6,7,8,9,10],\n",
    "    'criterion':[\"gini\",\"entropy\"],'random_state': [100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=5, random_state=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=5, min_samples_split=5, random_state=100, total=  17.4s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=5, random_state=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=5, min_samples_split=5, random_state=100, total=  17.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=5, random_state=100, total=  18.2s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=5, random_state=100, total=  17.2s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=5, random_state=100, total=  17.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=6, random_state=100, total=  17.2s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=6, random_state=100, total=  17.9s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=6, random_state=100, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=6, random_state=100, total=   9.8s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=6, random_state=100, total=  10.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=7, random_state=100, total=  10.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=7, random_state=100, total=  10.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=7, random_state=100, total=  15.8s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=7, random_state=100, total=  10.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=7, random_state=100, total=   9.9s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=8, random_state=100, total=  10.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=8, random_state=100, total=  10.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=8, random_state=100, total=  10.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=8, random_state=100, total=  12.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=8, random_state=100, total=  11.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=9, random_state=100, total=  10.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=9, random_state=100, total=   9.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=9, random_state=100, total=   9.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=9, random_state=100, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=9, random_state=100, total=  10.5s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=10, random_state=100, total=   9.8s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=10, random_state=100, total=   9.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=10, random_state=100, total=   9.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=10, random_state=100, total=   9.7s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_split=10, random_state=100, total=   9.6s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=5, random_state=100, total=  11.7s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=5, random_state=100, total=  11.8s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=5, random_state=100, total=  11.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=5, random_state=100, total=  11.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=5, random_state=100, total=  12.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=6, random_state=100, total=  12.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=6, random_state=100, total=  11.8s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=6, random_state=100, total=  11.9s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=6, random_state=100, total=  12.1s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=6, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=7, random_state=100, total=  13.5s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=7, random_state=100, total=  12.9s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=7, random_state=100, total=  12.0s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=7, random_state=100, total=  12.3s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=7, random_state=100, total=  13.0s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=8, random_state=100, total=  13.1s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=8, random_state=100, total=  12.3s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=8, random_state=100, total=  12.5s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=8, random_state=100, total=  11.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=8, random_state=100, total=  12.1s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=9, random_state=100, total=  11.6s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=9, random_state=100, total=  12.8s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=9, random_state=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=6, min_samples_split=9, random_state=100, total=  12.7s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=9, random_state=100, total=  11.8s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=9, random_state=100, total=  12.7s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, random_state=100, total=  13.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, random_state=100, total=  16.1s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, random_state=100, total=  13.4s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, random_state=100, total=  12.3s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, random_state=100, total=  12.4s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=5, random_state=100, total=  13.8s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=5, random_state=100, total=  13.5s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=5, random_state=100, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=5, random_state=100, total=  13.9s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=5, random_state=100, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=6, random_state=100, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=6, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=6, random_state=100, total=  14.1s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=6, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=6, random_state=100, total=  14.3s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=7, random_state=100, total=  15.2s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=7, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=7, random_state=100, total=  13.7s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=7, random_state=100, total=  13.5s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=7, random_state=100, total=  13.2s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=8, random_state=100, total=  13.7s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=8, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=8, random_state=100, total=  13.3s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=8, random_state=100, total=  13.2s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=8, random_state=100, total=  13.1s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=9, random_state=100, total=  13.2s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=9, random_state=100, total=  13.8s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=9, random_state=100, total=  14.4s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=9, random_state=100, total=  13.1s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=9, random_state=100, total=  13.3s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=10, random_state=100, total=  13.9s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=10, random_state=100, total=  13.4s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=10, random_state=100, total=  13.3s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=10, random_state=100, total=  15.2s\n",
      "[CV] criterion=gini, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=7, min_samples_split=10, random_state=100, total=  13.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=5, random_state=100, total=  15.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=5, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=5, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=5, random_state=100, total=  14.9s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=5, random_state=100, total=  14.7s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=6, random_state=100, total=  15.0s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=6, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=6, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=6, random_state=100, total=  14.7s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=6, random_state=100, total=  15.3s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=7, random_state=100, total=  15.2s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=7, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=7, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=7, random_state=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=8, min_samples_split=7, random_state=100, total=  15.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=7, random_state=100, total=  14.7s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=8, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=8, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=8, random_state=100, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=8, random_state=100, total=  15.5s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=8, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=9, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=9, random_state=100, total=  15.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=9, random_state=100, total=  15.1s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=9, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=9, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=10, random_state=100, total=  15.2s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=10, random_state=100, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=10, random_state=100, total=  15.3s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=10, random_state=100, total=  14.8s\n",
      "[CV] criterion=gini, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=gini, max_depth=8, min_samples_split=10, random_state=100, total=  14.6s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=5, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=5, random_state=100, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=5, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=5, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=5, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=6, random_state=100, total=  15.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=6, random_state=100, total=  15.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=6, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=6, random_state=100, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=6, random_state=100, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=7, random_state=100, total=  16.3s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=7, random_state=100, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=7, random_state=100, total=  15.3s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=7, random_state=100, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=7, random_state=100, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=8, random_state=100, total=  15.3s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=8, random_state=100, total=  15.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=8, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=8, random_state=100, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=8, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=9, random_state=100, total=  16.3s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=9, random_state=100, total=  17.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=9, random_state=100, total=  15.4s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=9, random_state=100, total=  15.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=9, random_state=100, total=  15.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=10, random_state=100, total=  15.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=10, random_state=100, total=  15.0s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=10, random_state=100, total=  15.8s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=10, random_state=100, total=  15.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_split=10, random_state=100, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=5, random_state=100, total=  18.0s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=5, random_state=100, total=  21.6s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=5, random_state=100, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=5, random_state=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=5, random_state=100, total=  19.3s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=5, random_state=100, total=  20.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=6, random_state=100, total=  18.2s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=6, random_state=100, total=  17.7s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=6, random_state=100, total=  18.2s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=6, random_state=100, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=6, random_state=100, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=7, random_state=100, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=7, random_state=100, total=  18.2s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=7, random_state=100, total=  19.6s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=7, random_state=100, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=7, random_state=100, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=8, random_state=100, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=8, random_state=100, total=  19.1s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=8, random_state=100, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=8, random_state=100, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=8, random_state=100, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=9, random_state=100, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=9, random_state=100, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=9, random_state=100, total=  18.5s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=9, random_state=100, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=9, random_state=100, total=  17.7s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, random_state=100, total=  18.1s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, random_state=100, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, random_state=100, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, random_state=100, total=  21.6s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, random_state=100, total=  20.1s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=5, random_state=100, total=  28.8s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=5, random_state=100, total=  21.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=5, random_state=100, total=  21.2s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=5, random_state=100, total=  21.6s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=5, random_state=100, total=  21.6s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=6, random_state=100, total=  22.0s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=6, random_state=100, total=  24.5s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=6, random_state=100, total=  22.3s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=6, random_state=100, total=  25.2s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=6, random_state=100, total=  23.4s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=7, random_state=100, total=  24.8s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=7, random_state=100, total=  22.3s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=7, random_state=100, total=  26.8s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=7, random_state=100, total=  32.4s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=7, random_state=100, total=  22.4s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=8, random_state=100, total=  35.0s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=8, random_state=100, total=  23.1s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=8, random_state=100, total=  47.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=8, random_state=100, total=  38.5s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=8, random_state=100, total=  28.8s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=9, random_state=100, total=  31.3s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=9, random_state=100, total=  22.2s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=9, random_state=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=9, random_state=100, total=  23.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=9, random_state=100, total=  22.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=9, random_state=100, total=  22.6s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=10, random_state=100, total=  21.2s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=10, random_state=100, total=  20.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=10, random_state=100, total=  21.9s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=10, random_state=100, total=  20.8s\n",
      "[CV] criterion=entropy, max_depth=7, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=7, min_samples_split=10, random_state=100, total=  20.9s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=5, random_state=100, total=  24.1s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=5, random_state=100, total=  23.8s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=5, random_state=100, total=  23.6s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=5, random_state=100, total=  23.8s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=5, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=5, random_state=100, total=  24.0s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=6, random_state=100, total=  23.6s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=6, random_state=100, total=  27.2s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=6, random_state=100, total=  32.9s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=6, random_state=100, total=  31.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=6, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=6, random_state=100, total=  26.9s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=7, random_state=100, total=  26.4s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=7, random_state=100, total=  23.4s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=7, random_state=100, total=  24.8s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=7, random_state=100, total=  24.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=7, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=7, random_state=100, total=  23.3s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=8, random_state=100, total=  24.1s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=8, random_state=100, total=  24.0s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=8, random_state=100, total=  23.6s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=8, random_state=100, total=  23.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=8, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=8, random_state=100, total=  25.6s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=9, random_state=100, total=  23.3s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=9, random_state=100, total=  24.0s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=9, random_state=100, total=  23.3s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=9, random_state=100, total=  24.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=9, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=9, random_state=100, total=  24.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=10, random_state=100, total=  23.7s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=10, random_state=100, total=  25.1s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=10, random_state=100, total=  26.9s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=10, random_state=100, total=  29.0s\n",
      "[CV] criterion=entropy, max_depth=8, min_samples_split=10, random_state=100 \n",
      "[CV]  criterion=entropy, max_depth=8, min_samples_split=10, random_state=100, total=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 69.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [5, 6, 7, 8],\n",
       "                         'min_samples_split': [5, 6, 7, 8, 9, 10],\n",
       "                         'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf=GridSearchCV(RandomForestClassifier(),param_grid=param_rf,refit=True,verbose=2)\n",
    "grid_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 8,\n",
       " 'min_samples_split': 6,\n",
       " 'random_state': 100}"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Parameters from the given range - \n",
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score on Train set using the best estimator- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857989816558555"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score on Test set using the best estimator- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480673773293189"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= grid_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.85      0.80      7477\n",
      "         1.0       0.72      0.58      0.64      4812\n",
      "\n",
      "    accuracy                           0.75     12289\n",
      "   macro avg       0.74      0.72      0.72     12289\n",
      "weighted avg       0.75      0.75      0.74     12289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report- \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6389 1088]\n",
      " [2008 2804]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix \n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7185984928455021"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC-AUC score \n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7204522096608428"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision - \n",
    "precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5827098919368247"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall score - \n",
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The scores observed are very similar with the difference among them being in the acceptable range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the feature Importance values of the RandomForest model- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp= pd.Series(grid_rf.best_estimator_.feature_importances_,index= X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the top 10 features in a horizontal bar chart - \n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x =feature_imp.sort_values(ascending = False).values[:10], y =feature_imp.sort_values(ascending = False).index[:10] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important features - \n",
    "From the plot shown above, we get a fair idea of the most important features of the data set. Almost all the extracted features are sitting at the top of the list. The vector dimensions have low amount of significance, which means the word2vec model did not perform well. Small number of words in corpora could be one of the reasons, also number of the dimensions can be tweaked as well . Also , pretrained model such as GLOVE can be used to further imporve the performance. \n",
    "Also different complex models and ensembles can be deployed to get more sophisticated results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing functions to apply predictions on individual test files - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling a single test file - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_file(path):  \n",
    "    \n",
    "    sent=[]\n",
    "    f = open(path)\n",
    "    n = \"\"\n",
    "    for line in f: \n",
    "        n += line\n",
    "    \n",
    "    sent.append(n)\n",
    "    \n",
    "    return sent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store sentence wise titles - (Input similar to read_data func) for test file \n",
    "\n",
    "def read_data_summary_test(path):\n",
    "  \n",
    "    summary_dict = {}\n",
    "    sent=''\n",
    "    ad= 1\n",
    "    f = open(path)\n",
    "    #n = []\n",
    "    for line in f: \n",
    "        sent = sent+line\n",
    "        \n",
    "        \n",
    "    summary_dict[int(ad)] = [re.sub('\"','',i) for i in sent.split(\".\") if i !='']\n",
    "    \n",
    "    return (summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operations(num_features,path_test,path_summary,word2vec_model,model_features,classifier):\n",
    "    \n",
    "    summary_dict_test = read_data_summary_test(path_summary)\n",
    "    \n",
    "    \n",
    "    test_sent = prepare_test_file(path_test)\n",
    "    \n",
    "    them_dict_test,token_dict_test,word_count_dict_test = token_func(test_sent)  \n",
    "    \n",
    "    main_data_test,sent_count_dict_test= create_dataframe(test_sent,summary_dict_test)\n",
    "    \n",
    "    remove_null(main_data_test,sent_count_dict_test)\n",
    "    \n",
    "    top_noun_dict_test= top_nouns(token_dict_test)\n",
    "    \n",
    "    \n",
    "    features_(main_data_test,sent_count_dict_test)\n",
    "    \n",
    "    \n",
    "    w2v_feature_array = averaged_word_vectorizer(corpus=main_data_test['Sent_tokens'], model=word2vec_model,   #function to get vectors\n",
    "                                             num_features=num_features)\n",
    "    vector_df_test= pd.DataFrame(w2v_feature_array)\n",
    "    \n",
    "    \n",
    "    main_data_test = main_data_test.merge(vector_df_test,left_index=True, right_index=True)\n",
    "    \n",
    "    centroid_dict,title_vector_dict = centroid_vector(main_data_test)\n",
    "    \n",
    "    sent_simlilarity(main_data_test)\n",
    "    \n",
    "    sent_similar_norm(main_data_test)\n",
    "    \n",
    "    main_data_test.reset_index(inplace=True,drop=True)\n",
    "    main_data_test =main_data_test.fillna(0)\n",
    "    \n",
    "    X = main_data_test[model_features] \n",
    "    X.drop('Summary',axis=1,inplace=True)\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    predictions = classifier.predict(X)\n",
    "    \n",
    "    show_data = main_data_test[['Sentence','Summary']]\n",
    "    show_data['Prediction'] = predictions\n",
    "    \n",
    "    return (show_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the developed functions using a sample file- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"BBC News Summary/BBC News Summary/News Articles/business/013.txt\"\n",
    "path_summary =\"BBC News Summary/BBC News Summary/Summaries/business/013.txt\"\n",
    "model_features = [i for i in main_data_sport.columns if i not in ['Sentence','Title','Sentence_len',\n",
    "                                                                 'Sent_no','Sent_tokens','Thematic_count'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = operations(path_test =path_test,path_summary=path_summary ,word2vec_model =word2vec ,num_features=100,\n",
    "                         model_features = model_features,classifier=grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 3)"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result data frame obtained after using the pretrained model for predictions- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Struggling Japanese car maker Mitsubishi Motors has struck a deal to supply French car maker Peugeot with 30,000 sports utility vehicles (SUV)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The two firms signed a Memorandum of Understanding, and say they expect to seal a final agreement by Spring 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The alliance comes as a badly-needed boost for loss-making Mitsubishi, after several profit warnings and poor sales</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The SUVs will be built in Japan using Peugeot's diesel engines and sold mainly in the European market</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falling sales have left Mitsubishi Motors with underused capacity, and the production deal with Peugeot gives it a chance to utilise some of it</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In January, Mitsubishi Motors issued its third profits warning in nine months, and cut its sales forecasts for the year to March 2005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Its sales have slid 41% in the past year, catalysed by the revelation that the company had systematically been hiding records of faults and then secretly repairing vehicles</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As a result, the Japanese car maker has sought a series of financial bailouts</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Last month it said it was looking for a further 540bn yen ($5.2bn; 2.77bn) in fresh financial backing, half of it from other companies in the Mitsubishi group</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US-German carmaker DaimlerChrylser, a 30% shareholder in Mitsubishi Motors, decided in April 2004 not to pump in any more money</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The deal with Peugeot was celebrated by Mitsubishi's newly-appointed chief executive Takashi Nishioka, who took over after three top bosses stood down last month to shoulder responsibility for the firm's troubles</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mitsubishi Motors has forecast a net loss of 472bn yen in its current financial year to March 2005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Last month, it signed a production agreement with Japanese rival Nissan Motor to supply it with 36,000 small cars for sale in Japan</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It has been making cars for Nissan since 2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                Sentence  \\\n",
       "0                                                                         Struggling Japanese car maker Mitsubishi Motors has struck a deal to supply French car maker Peugeot with 30,000 sports utility vehicles (SUV)   \n",
       "1                                                                                                       The two firms signed a Memorandum of Understanding, and say they expect to seal a final agreement by Spring 2005   \n",
       "2                                                                                                    The alliance comes as a badly-needed boost for loss-making Mitsubishi, after several profit warnings and poor sales   \n",
       "3                                                                                                                  The SUVs will be built in Japan using Peugeot's diesel engines and sold mainly in the European market   \n",
       "4                                                                        Falling sales have left Mitsubishi Motors with underused capacity, and the production deal with Peugeot gives it a chance to utilise some of it   \n",
       "5                                                                                  In January, Mitsubishi Motors issued its third profits warning in nine months, and cut its sales forecasts for the year to March 2005   \n",
       "6                                           Its sales have slid 41% in the past year, catalysed by the revelation that the company had systematically been hiding records of faults and then secretly repairing vehicles   \n",
       "7                                                                                                                                          As a result, the Japanese car maker has sought a series of financial bailouts   \n",
       "8                                                       Last month it said it was looking for a further 540bn yen ($5.2bn; 2.77bn) in fresh financial backing, half of it from other companies in the Mitsubishi group   \n",
       "9                                                                                        US-German carmaker DaimlerChrylser, a 30% shareholder in Mitsubishi Motors, decided in April 2004 not to pump in any more money   \n",
       "10  The deal with Peugeot was celebrated by Mitsubishi's newly-appointed chief executive Takashi Nishioka, who took over after three top bosses stood down last month to shoulder responsibility for the firm's troubles   \n",
       "11                                                                                                                    Mitsubishi Motors has forecast a net loss of 472bn yen in its current financial year to March 2005   \n",
       "12                                                                                   Last month, it signed a production agreement with Japanese rival Nissan Motor to supply it with 36,000 small cars for sale in Japan   \n",
       "13                                                                                                                                                                         It has been making cars for Nissan since 2003   \n",
       "\n",
       "    Summary  Prediction  \n",
       "0         1         1.0  \n",
       "1         0         0.0  \n",
       "2         0         0.0  \n",
       "3         0         0.0  \n",
       "4         1         1.0  \n",
       "5         1         0.0  \n",
       "6         0         0.0  \n",
       "7         1         1.0  \n",
       "8         0         0.0  \n",
       "9         0         0.0  \n",
       "10        0         0.0  \n",
       "11        1         0.0  \n",
       "12        1         1.0  \n",
       "13        0         0.0  "
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.to_csv('business_013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF FILE !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
